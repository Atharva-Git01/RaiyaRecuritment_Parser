# Intelligent Resume Transformation Agent â€” PlantUML Workflow Diagram

## Powered by LangGraph & Azure OpenAI Phi-4 LLM

---

## Complete End-to-End Pipeline Workflow

```plantuml
@startuml Intelligent_Resume_Transformation_Agent_Pipeline

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  THEME & STYLING
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
skinparam backgroundColor #FEFEFE
skinparam shadowing true
skinparam defaultFontName "Segoe UI"
skinparam defaultFontSize 12

skinparam activity {
    BackgroundColor #E8F0FE
    BorderColor #1A73E8
    FontColor #202124
    DiamondBackgroundColor #FFF3E0
    DiamondBorderColor #E65100
    DiamondFontColor #BF360C
}

skinparam partition {
    BackgroundColor #F8F9FA
    BorderColor #DADCE0
    FontColor #202124
    FontSize 14
    FontStyle bold
}

skinparam note {
    BackgroundColor #FFFDE7
    BorderColor #F9A825
    FontColor #333333
    FontSize 10
}

title <size:20><b>Intelligent Resume Transformation Agent â€” Complete Workflow</b></size>\n<size:14>LangGraph + Multi-Node Reasoning Graph + Azure OpenAI Phi-4</size>

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  START
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
|#E3F2FD|Main Pipeline (LangGraph StateGraph)|

start

:ğŸ“„ **Resume File Input**
  (PDF / DOCX / TXT / RTF);

note right
  **ResumeState initialized:**
  â€¢ file_path: str
  â€¢ file_type: str
  â€¢ status: "processing"
  â€¢ messages: []
end note

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 0: INITIALIZATION
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
partition "**Phase 0: Initialization**" {
    :âš™ï¸ **Load Environment Variables** (.env)
    â”€â”€ AZURE_OPENAI_API_KEY
    â”€â”€ AZURE_OPENAI_ENDPOINT
    â”€â”€ AZURE_OPENAI_API_VERSION
    â”€â”€ AZURE_OPENAI_DEPLOYMENT_NAME = "phi-4";

    :ğŸ”Œ **Initialize AzureOpenAI Client**
    (temperature=0, strict JSON mode);

    :ğŸ”— **Create LangGraph StateGraph**
    Compile workflow with 5 nodes:
    parse â†’ reasoning_extract â†’ validate_enrich
    â†’ human_review â†’ store â†’ END;
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 1: STATE SCHEMA
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
partition "**Phase 1: State Schema Definition**" {
    :ğŸ“‹ **Define ResumeState (TypedDict)**
    â”€â”€ file_path, file_type
    â”€â”€ raw_text, parse_error
    â”€â”€ extraction_metadata
    â”€â”€ structured_data, extract_error
    â”€â”€ validated_data, validation_error
    â”€â”€ human_approved, human_review_notes
    â”€â”€ database_id, store_error
    â”€â”€ status, messages[];
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 2: PARSE AGENT
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

|#E8F5E9|Parse Agent (Agent 1)|

partition "**Phase 2: Parse Agent â€” Compulsory Delegate to UniversalExtractor**" {

    :ğŸ” **parse_agent(state)**
    LangGraph Node
    Reads: state['file_path'];

    note right
      **COMPULSORY DELEGATION CONTRACT**
      â€¢ NO standalone parse_pdf(), parse_docx(), parse_txt()
      â€¢ ALL extraction via UniversalExtractor only
      â€¢ Single entry point: extract_universal(file_path)
    end note

    :ğŸ“‚ **Add extraction pipeline to sys.path**
    modularized_resume_extraction_normalization/;

    :ğŸ—ï¸ **Initialize UniversalExtractor**
    _universal_extractor = UniversalExtractor(
        use_gpu=False, use_neural_ocr=False
    );

    ' â”€â”€ UniversalExtractor Internal Flow â”€â”€

    |#C8E6C9|UniversalExtractor (Compulsory Delegate)|

    :ğŸ” **Step 0: File Identity Hash**
    SHA-256 of raw file bytes;

    :ğŸ” **Step 1: Layout Detection**
    LayoutDetector.detect_multi_layout(file_path)
    â”€â”€ Page count analysis
    â”€â”€ Column/sidebar pattern detection
    â”€â”€ Image-based vs text-based classification;

    if (Multi-page AND\nmulti-layout?\n(sidebar / multi-column)) then (yes)
        :ğŸ“ **_extract_multi_layout_custom()**
        â”€â”€ ColumnExtractor per page
        â”€â”€ _classify_content_sides() (main vs sidebar)
        â”€â”€ _extract_candidate_name();

        note right
          Components:
          â€¢ **ColumnExtractor** (column_extractor.py)
          â€¢ **HeadingFormatter** (heading_formatter.py)
          â€¢ L/R column separation per page
        end note
    else (no)
        :ğŸ“„ **StandardExtractor.extract()**
        â”€â”€ docstrange DocumentExtractor
        (single-page any layout OR
        multi-page single-column);

        note left
          Components:
          â€¢ **StandardExtractor** (standard_extraction.py)
          â€¢ docstrange pipeline
        end note
    endif

    :ğŸ” **Step 3: Fingerprinting**
    â”€â”€ content_hash = SHA-256(get_raw_extracted_text())
    â”€â”€ section_hashes = per-section SHA-256 digests;

    :ğŸ“¦ **ExtractionResult**
    â”€â”€ .content / .main_content / .sidebar_content
    â”€â”€ .layout_type, .pages, .is_image_based
    â”€â”€ .file_hash, .content_hash, .section_hashes
    â”€â”€ .get_raw_extracted_text() â†’ plain text
    â”€â”€ .to_enhanced_markdown() â†’ formatted md;

    |#E8F5E9|Parse Agent (Agent 1)|

    if (raw_text is empty\nor None?) then (yes)
        :âŒ **ValueError**
        "UniversalExtractor returned empty text";
        :state['parse_error'] = error message
        state['raw_text'] = None;
    else (no)
        :âœ… **Populate State**
        state['raw_text'] = result.get_raw_extracted_text()
        state['parse_error'] = None
        state['extraction_metadata'] = {
            layout_type, pages, is_image_based,
            extraction_method, word_count,
            file_hash, content_hash,
            section_hashes, enhanced_markdown
        };
    endif
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 3: REASONING EXTRACT AGENT
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

|#FFF3E0|Reasoning Extract Agent (Agent 2)|

partition "**Phase 3: Reasoning Extract Agent â€” Multi-Node Reasoning Graph**" {

    :ğŸ§  **reasoning_extract_agent(state)**
    LangGraph Node
    Source: pipeline_bridge.py;

    note right
      **Drop-in replacement** for old
      monolithic Extract Agent.
      LLM used **only** for classification
      (not full extraction).
    end note

    if (parse_error exists?) then (yes)
        :âŒ Set extract_error
        "Cannot extract: parse failed"
        Return state;
        detach
    else (no)
    endif

    ' â”€â”€ Step 1: Adapter In â”€â”€

    |#FFE0B2|Extraction Adapter (In)|

    :ğŸ”„ **extraction_adapter.py**
    extraction_result_to_reasoning_state(raw_text, content_hash)
    â”€â”€ split_sections() â€” regex-based section splitting
    â”€â”€ extract_roles_deterministic() â€” pattern-matched
    â”€â”€ extract_skills_deterministic() â€” delimiter-based
    â”€â”€ Contact parsing â€” regex (email, phone, LinkedIn, GitHub);

    note right
      Produces **ReasoningState** TypedDict:
      â€¢ content_hash, raw_text
      â€¢ sections, roles, skills
      â€¢ normalized_skills, timeline_analysis
      â€¢ ambiguities, confidence_flags
      â€¢ audit_log
    end note

    ' â”€â”€ Step 2: 6-Node Sub-Graph â”€â”€

    |#FFCC80|6-Node Reasoning Sub-Graph|

    :ğŸƒ **graph_runner.py**
    run_reasoning_graph(reasoning_state);

    ' â”€â”€ Node 1 â”€â”€
    :ğŸ”’ **Node 1: section_authority_node** âŒ No LLM
    (deterministic)
    â”€â”€ Reads: state['sections'], state['roles']
    â”€â”€ Validates: experience claims from correct sections
    â”€â”€ Writes: state['roles'] + authority_flag
    â”€â”€ Source: nodes/section_authority_node.py;

    note right #E8F5E9
      **Deterministic**
      No LLM call
    end note

    ' â”€â”€ Conditional check â”€â”€
    if (Experience data\nexists in roles?) then (yes)

        ' â”€â”€ Node 2 â”€â”€
        :â±ï¸ **Node 2: timeline_reasoner_node** âŒ No LLM
        (deterministic â€” pure date math)
        â”€â”€ Reads: state['roles'] with dates
        â”€â”€ Computes: date overlaps, total experience
        â”€â”€ Detects: parallel employment inflation
        â”€â”€ Writes: state['timeline_analysis'] = {
            total_months, overlapping_roles,
            gaps, parallel_employment_flags
        }
        â”€â”€ Source: nodes/timeline_reasoner_node.py;

        note right #E8F5E9
          **Deterministic**
          No LLM call
        end note

    else (no â€” skip)
        :âš¡ Skip timeline_reasoner
        (no experience data);

        note right
          **transitions.py** handles
          conditional node skipping
        end note
    endif

    ' â”€â”€ Node 3 â”€â”€
    :ğŸ”— **Node 3: skill_evidence_node** âš ï¸ Optional LLM
    (deterministic + optional LLM assist)
    â”€â”€ Reads: state['skills'], state['roles']
    â”€â”€ Maps: each skill â†’ role(s) where used
    â”€â”€ Finds: skills listed but never used in roles
    â”€â”€ Writes: state['normalized_skills'] + evidence
    â”€â”€ Source: nodes/skill_evidence_node.py;

    note right #FFF9C4
      **Deterministic** with
      optional LLM for
      ambiguous mappings
    end note

    ' â”€â”€ Node 4 â”€â”€
    :ğŸ” **Node 4: ambiguity_detector_node** âœ… LLM
    (LLM classification â€” <200 tokens)
    â”€â”€ Reads: state['roles'] responsibilities text
    â”€â”€ Sends: pre-extracted phrases to LLM
    â”€â”€ Detects: "worked on", "familiar with",
       "knowledge of", "exposure to"
    â”€â”€ Writes: state['ambiguities'] = [flagged phrases]
    â”€â”€ Prompt: llm/prompts/ambiguity_prompt.txt
    â”€â”€ Source: nodes/ambiguity_detector_node.py;

    note right #FFCDD2
      **LLM Classification** via
      Azure OpenAI Phi-4
      (temp=0, strict JSON)
    end note

    ' â”€â”€ Node 5 â”€â”€
    :ğŸ·ï¸ **Node 5: classification_node** âœ… LLM
    (LLM classification â€” strict JSON, <200 tokens)
    â”€â”€ Reads: state['normalized_skills'] + context
    â”€â”€ Sends: skill list + context to LLM
    â”€â”€ Tags: professional / academic /
       cert-only / tool-only
    â”€â”€ Writes: state['confidence_flags']
    â”€â”€ Prompt: llm/prompts/classification_prompt.txt
    â”€â”€ Source: nodes/classification_node.py;

    note right #FFCDD2
      **LLM Classification** via
      Azure OpenAI Phi-4
      (temp=0, strict JSON)
    end note

    ' â”€â”€ Node 6 â”€â”€
    :ğŸ” **Node 6: finalization_node** âŒ No LLM
    (deterministic)
    â”€â”€ Reads: entire ReasoningState
    â”€â”€ Locks: state (no further mutations)
    â”€â”€ Appends: complete audit trail entry
    â”€â”€ Returns: finalized ReasoningState
    â”€â”€ Source: nodes/finalization_node.py;

    note right #E8F5E9
      **Deterministic**
      No LLM call
    end note

    ' â”€â”€ Validators run after each node â”€â”€
    :ğŸ›¡ï¸ **Validators (run after every node)**
    â”€â”€ schema_validator.py â†’ schema enforcement
    â”€â”€ hallucination_detector.py â†’ flag invented data
    â”€â”€ consistency_checker.py â†’ cross-validate data;

    ' â”€â”€ Step 3: Adapter Out â”€â”€

    |#FFE0B2|Extraction Adapter (Out)|

    :ğŸ”„ **extraction_adapter.py**
    reasoning_state_to_structured_data(final_state)
    â”€â”€ Maps ReasoningState â†’ structured_data format
    â”€â”€ Adds reasoning metadata
       (timeline, ambiguities, confidence_flags, audit_log)
    â”€â”€ Compatible with validate_and_enrich_agent;

    |#FFF3E0|Reasoning Extract Agent (Agent 2)|

    :âœ… **Populate State**
    state['structured_data'] = {
        contact: {name, email, phone, ...},
        summary, experience[], education[],
        skills[], certifications[],
        timeline_analysis, ambiguities,
        confidence_flags, audit_log
    }
    state['extract_error'] = None;
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 4: VALIDATE & ENRICH AGENT
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

|#F3E5F5|Validate & Enrich Agent (Agent 3)|

partition "**Phase 4: Validate & Enrich Agent â€” Deterministic Python**" {

    :âœ¨ **validate_and_enrich_agent(state)**
    LangGraph Node
    Reads: state['structured_data'];

    if (extract_error exists?) then (yes)
        :âŒ Set validation_error
        "Cannot validate: extraction failed";
        detach
    else (no)
    endif

    :ğŸ§¹ **1. clean_contact_info(contact)**
    â”€â”€ Validate email (must contain @)
    â”€â”€ Clean phone (strip non-numeric except +/-/()/ )
    â”€â”€ Strip whitespace from all fields;

    :ğŸ“… **2. calculate_experience(experience_list)**
    â”€â”€ parse_date() for start/end dates
    â”€â”€ Sum months across all positions
    â”€â”€ Convert to years (rounded to 1 decimal)
    â”€â”€ "Present" â†’ datetime.now();

    :ğŸ”§ **3. standardize_skills(skills)**
    â”€â”€ Lowercase lookup against STANDARD_SKILLS taxonomy
    â”€â”€ Deduplicate ("reactjs" + "react" â†’ "React")
    â”€â”€ Sort alphabetically;

    :ğŸ“ **4. Add Metadata**
    â”€â”€ processed_date = ISO-8601 timestamp
    â”€â”€ data_version = "1.0";

    :âœ… **Populate State**
    state['validated_data'] = enriched JSON
    state['validation_error'] = None;
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 5: HUMAN REVIEW (HITL)
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

|#E0F2F1|Human Review Agent (Agent 3.5 â€” HITL)|

partition "**Phase 5: Human Review Agent â€” Visual Grounding (HITL)**" {

    :ğŸ‘ï¸ **human_review_agent(state)**
    LangGraph Node
    Reads: state['validated_data'], state['file_path'];

    if (validation_error exists?) then (yes)
        :âš ï¸ Skip review
        state['human_approved'] = False;
        detach
    else (no)
    endif

    :ğŸ–¼ï¸ **1. Generate Resume Image**
    pdf2image â†’ base64 PNG;

    :ğŸŒ **2. Build HTML Review Interface**
    â”€â”€ Left panel: Original resume image (zoomable)
    â”€â”€ Right panel: Editable extracted fields
       (contact, skills, experience, education);

    :ğŸ“‚ **3. Write HTML to temp file**
    tempfile.NamedTemporaryFile(.html);

    :ğŸŒ **4. Open in Browser**
    webbrowser.open(html_file);

    :â³ **5. Wait for Reviewer Decision**
    Console input: "approve" or "reject";

    if (Reviewer Decision?) then (âœ… APPROVED)
        :âœ… **Merge Edits**
        â”€â”€ Apply any field corrections
        â”€â”€ state['human_approved'] = True
        â”€â”€ state['human_review_notes'] = "...";
    else (âŒ REJECTED)
        :âŒ **Record Rejection**
        â”€â”€ state['human_approved'] = False
        â”€â”€ state['human_review_notes'] = rejection reason;
    endif
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 6: STORE AGENT
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

|#FCE4EC|Store Agent (Agent 4)|

partition "**Phase 6: Store Agent â€” SQLite Database**" {

    :ğŸ’¾ **store_agent(state)**
    LangGraph Node
    Reads: state['validated_data'], state['human_approved'];

    if (validation_error exists?) then (yes)
        :âŒ Set store_error
        "Cannot store: validation failed"
        status = 'failed';
        detach
    else (no)
    endif

    if (human_approved == False?) then (yes â€” rejected)
        :âš ï¸ **Skip Storage**
        state['status'] = 'rejected'
        state['store_error'] = "Rejected by reviewer";
    else (no â€” approved)

        :ğŸ—„ï¸ **initialize_database()**
        CREATE TABLE IF NOT EXISTS:
        â”€â”€ candidates (PK: id)
        â”€â”€ experience (FK: candidate_id)
        â”€â”€ education (FK: candidate_id)
        â”€â”€ skills (FK: candidate_id)
        â”€â”€ certifications (FK: candidate_id);

        :ğŸ’¿ **INSERT Records**
        â”€â”€ INSERT INTO candidates â†’ candidate_id
        â”€â”€ INSERT INTO experience Ã— N entries
        â”€â”€ INSERT INTO education Ã— N entries
        â”€â”€ INSERT INTO skills Ã— N entries
        â”€â”€ INSERT INTO certifications Ã— N entries;

        :âœ… **Populate State**
        state['database_id'] = candidate_id
        state['status'] = 'completed';
    endif
}

' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
'  PHASE 7-9: RESULTS & QUERY
' â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

|#E3F2FD|Main Pipeline (LangGraph StateGraph)|

partition "**Phase 7â€“9: Results Summary & Query**" {

    :ğŸ“Š **Workflow Summary**
    Print all messages from state['messages']
    â”€â”€ Final Status: COMPLETED / REJECTED / FAILED
    â”€â”€ Database ID (if stored)
    â”€â”€ Human Review: âœ“ APPROVED / âœ— REJECTED;

    :ğŸ” **query_database()**
    SELECT from candidates
    ORDER BY created_at DESC;
}

stop

@enduml
```

---

## Reasoning Sub-Graph Internal Flow (Detailed)

```plantuml
@startuml Reasoning_SubGraph_Internal_Flow

skinparam backgroundColor #FEFEFE
skinparam shadowing true
skinparam defaultFontName "Segoe UI"
skinparam defaultFontSize 11

skinparam activity {
    BackgroundColor #FFF8E1
    BorderColor #F57F17
    FontColor #212121
    DiamondBackgroundColor #E3F2FD
    DiamondBorderColor #1565C0
}

skinparam partition {
    BackgroundColor #FAFAFA
    BorderColor #BDBDBD
    FontColor #212121
    FontSize 13
    FontStyle bold
}

skinparam note {
    BackgroundColor #E8F5E9
    BorderColor #43A047
    FontColor #1B5E20
    FontSize 10
}

title <size:18><b>6-Node Reasoning Sub-Graph â€” Internal Flow</b></size>\n<size:12>resume_reasoning_agent/ | 4 Deterministic + 2 LLM Classification</size>

start

:ğŸ“¥ **Input from Parse Agent**
raw_text + content_hash
(via pipeline_bridge.py);

' â”€â”€ Adapter In â”€â”€
partition "**Extraction Adapter (In) â€” extraction_adapter.py**" {
    :ğŸ”„ **extraction_result_to_reasoning_state()**
    â”€â”€ split_sections() (regex)
    â”€â”€ extract_roles_deterministic() (patterns)
    â”€â”€ extract_skills_deterministic() (delimiters)
    â”€â”€ Contact parsing (regex: email, phone, LinkedIn, GitHub);

    :ğŸ“‹ **ReasoningState Initialized**
    â”€â”€ content_hash: str
    â”€â”€ raw_text: str
    â”€â”€ sections: Dict[str, str]
    â”€â”€ roles: List[Dict]
    â”€â”€ skills: List[str]
    â”€â”€ audit_log: [];
}

' â”€â”€ Reasoning Nodes â”€â”€
partition "**6-Node LangGraph Sub-Graph (graph_runner.py)**" {

    ' Node 1
    #C8E6C9:ğŸ”’ **Node 1: section_authority_node**
    [DETERMINISTIC â€” No LLM]
    â”€â”€ Validate experience claims â†” correct sections
    â”€â”€ Flag claims in wrong sections
    â”€â”€ Append authority_flag to each role
    â”€â”€ Audit: log section authority decisions;

    ' Transition check
    if (roles[] has date\ninformation?) then (yes)
        ' Node 2
        #C8E6C9:â±ï¸ **Node 2: timeline_reasoner_node**
        [DETERMINISTIC â€” Pure Date Math]
        â”€â”€ Compute overlapping roles
        â”€â”€ Calculate total_months
        â”€â”€ Detect parallel employment inflation
        â”€â”€ Identify gaps between roles
        â”€â”€ Write timeline_analysis;
    else (no data â€” skip)
        :âš¡ **transitions.py**
        Skip timeline_reasoner;
    endif

    ' Node 3
    #FFF9C4:ğŸ”— **Node 3: skill_evidence_node**
    [DETERMINISTIC + Optional LLM]
    â”€â”€ Map each skill â†’ role(s) description
    â”€â”€ Identify "skill-without-usage"
    â”€â”€ Optional LLM for ambiguous mappings
    â”€â”€ Write normalized_skills with evidence;

    ' Node 4
    #FFCDD2:ğŸ” **Node 4: ambiguity_detector_node**
    [LLM CLASSIFICATION â€” Azure OpenAI Phi-4]
    â”€â”€ Send pre-extracted phrases to LLM
    â”€â”€ Classify: vague/non-committal language
    â”€â”€ Prompt: ambiguity_prompt.txt (<200 tokens)
    â”€â”€ Write ambiguities[];
    note right
      **llm_client.py** wrapper:
      temp=0, JSON schema enforced
      **response_parser.py** validates
    end note

    ' Node 5
    #FFCDD2:ğŸ·ï¸ **Node 5: classification_node**
    [LLM CLASSIFICATION â€” Azure OpenAI Phi-4]
    â”€â”€ Send skills + context to LLM
    â”€â”€ Tag: professional / academic /
       cert-only / tool-only
    â”€â”€ Prompt: classification_prompt.txt (<200 tokens)
    â”€â”€ Write confidence_flags[];
    note right
      **llm_client.py** wrapper:
      temp=0, JSON schema enforced
      **response_parser.py** validates
    end note

    ' Node 6
    #C8E6C9:ğŸ” **Node 6: finalization_node**
    [DETERMINISTIC â€” No LLM]
    â”€â”€ Lock state (prevent further mutation)
    â”€â”€ Append complete audit trail entry
    â”€â”€ Return finalized ReasoningState;
}

' â”€â”€ Validators â”€â”€
partition "**Validators (run after every node)**" {
    :ğŸ›¡ï¸ **schema_validator.py**
    â”€â”€ No unknown keys, required fields present
    â”€â”€ Field type checks;

    :ğŸ•µï¸ **hallucination_detector.py**
    â”€â”€ Skills/roles not in raw_text â†’ flagged/removed;

    :ğŸ” **consistency_checker.py**
    â”€â”€ Experience years vs timeline computation
    â”€â”€ Skill claims vs role descriptions
    â”€â”€ Education dates vs work dates;
}

' â”€â”€ Adapter Out â”€â”€
partition "**Extraction Adapter (Out) â€” extraction_adapter.py**" {
    :ğŸ”„ **reasoning_state_to_structured_data()**
    â”€â”€ ReasoningState â†’ structured_data format
    â”€â”€ Include reasoning metadata
    â”€â”€ Compatible with validate_and_enrich_agent;
}

:ğŸ“¤ **Output â†’ state['structured_data']**
{contact, summary, experience[], education[],
skills[], certifications[], languages[],
timeline_analysis, ambiguities,
confidence_flags, audit_log};

stop

@enduml
```

---

## Error Propagation Flow

```plantuml
@startuml Error_Propagation_Flow

skinparam backgroundColor #FEFEFE
skinparam shadowing true
skinparam defaultFontName "Segoe UI"
skinparam defaultFontSize 12

skinparam activity {
    BackgroundColor #FFEBEE
    BorderColor #C62828
    FontColor #212121
    DiamondBackgroundColor #FFF3E0
    DiamondBorderColor #E65100
}

title <size:16><b>Error Propagation Through Agents</b></size>\n<size:11>Each agent checks upstream errors before proceeding</size>

start

partition "**Parse Agent**" {
    if (File read / extraction\nfails?) then (ERROR)
        :âŒ state['parse_error'] = error;
    else (OK)
        :âœ… state['raw_text'] populated;
    endif
}

partition "**Reasoning Extract Agent**" {
    if (parse_error exists?) then (YES)
        :âŒ state['extract_error'] =\n"Cannot extract: parse failed"
        **Return immediately**;
    else (NO)
        if (Sub-graph node\nfails?) then (ERROR)
            :âŒ state['extract_error'] = error
            audit_log records failed node;
        else (OK)
            :âœ… state['structured_data'] populated;
        endif
    endif
}

partition "**Validate & Enrich Agent**" {
    if (extract_error exists?) then (YES)
        :âŒ state['validation_error'] =\n"Cannot validate: extraction failed";
    else (NO)
        if (Data processing\nfails?) then (ERROR)
            :âŒ state['validation_error'] = error;
        else (OK)
            :âœ… state['validated_data'] populated;
        endif
    endif
}

partition "**Human Review Agent (HITL)**" {
    if (validation_error exists?) then (YES)
        :âš ï¸ state['human_approved'] = False
        Skip review;
    else (NO)
        :ğŸ‘ï¸ HTML Review Interface
        â”€â”€ Approve or Reject;
    endif
}

partition "**Store Agent**" {
    if (validation_error exists?) then (YES)
        :âŒ state['store_error']
        state['status'] = **'failed'**;
    else (NO)
        if (human_approved == False?) then (REJECTED)
            :âš ï¸ state['status'] = **'rejected'**
            Skip storage;
        else (APPROVED)
            if (SQLite insert\nfails?) then (ERROR)
                :âŒ state['store_error']
                state['status'] = **'failed'**;
            else (OK)
                :âœ… state['database_id'] = candidate_id
                state['status'] = **'completed'**;
            endif
        endif
    endif
}

stop

@enduml
```

---

## Cross-Pipeline Integration Diagram

```plantuml
@startuml Cross_Pipeline_Integration

skinparam backgroundColor #FEFEFE
skinparam shadowing true
skinparam defaultFontName "Segoe UI"
skinparam defaultFontSize 11

skinparam package {
    BackgroundColor #E3F2FD
    BorderColor #1565C0
    FontSize 14
    FontStyle bold
}

skinparam component {
    BackgroundColor #FFFFFF
    BorderColor #1565C0
    FontSize 11
}

skinparam database {
    BackgroundColor #FFF9C4
    BorderColor #F57F17
}

title <size:16><b>Cross-Pipeline Integration</b></size>\n<size:11>Pipeline 1 (Extraction) feeds into Pipeline 2 (Parsing Agent)</size>

package "**Pipeline 1: modularized_resume_extraction_normalization**\n(Deterministic Extraction â€” Docstrange + Layout Detection)" {
    [Resume File\n(PDF/DOCX/TXT/RTF)] as input
    [LayoutDetector\n(layout_detector.py)] as layout
    [ColumnExtractor\n(column_extractor.py)] as column
    [StandardExtractor\n(standard_extraction.py)] as standard
    [HeadingFormatter\n(heading_formatter.py)] as heading
    [Hashing\n(hashing.py)] as hashing
    [ExtractionResult\n(extraction_models.py)] as result
    [UniversalExtractor\n(universal_extractor.py)] as universal

    input --> universal
    universal --> layout
    layout --> column : multi-layout
    layout --> standard : standard layout
    column --> heading
    standard --> heading
    heading --> hashing
    hashing --> result
}

package "**Pipeline 2: modularized_resume_parsing_agent**\n(LangGraph + Azure OpenAI Phi-4)" {
    [Parse Agent\n(Compulsory Delegate)] as parse
    [Reasoning Extract Agent\n(6-Node Sub-Graph)] as reason

    package "Reasoning Sub-Graph Nodes" {
        [section_authority\n(deterministic)] as n1
        [timeline_reasoner\n(deterministic)] as n2
        [skill_evidence\n(determ. + opt. LLM)] as n3
        [ambiguity_detector\n(LLM classification)] as n4
        [classification\n(LLM classification)] as n5
        [finalization\n(deterministic)] as n6
    }

    [Validate & Enrich Agent\n(Deterministic Python)] as validate
    [Human Review Agent\n(HITL â€” HTML Interface)] as hitl
    [Store Agent\n(SQLite)] as store

    parse --> reason
    reason --> n1
    n1 --> n2
    n2 --> n3
    n3 --> n4
    n4 --> n5
    n5 --> n6
    n6 --> validate
    validate --> hitl
    hitl --> store
}

database "resume_ats.db\n(SQLite)" as db {
    [candidates]
    [experience]
    [education]
    [skills]
    [certifications]
}

cloud "Azure OpenAI\nPhi-4 Model" as llm

result ==> parse : get_raw_extracted_text()\ncontent_hash

n4 ..> llm : classification call\n(<200 tokens)
n5 ..> llm : classification call\n(<200 tokens)

store ==> db : INSERT INTO\n5 normalised tables

@enduml
```

---

## Database Schema Diagram

```plantuml
@startuml Database_Schema

skinparam backgroundColor #FEFEFE
skinparam shadowing true
skinparam defaultFontName "Segoe UI"

skinparam class {
    BackgroundColor #E8F0FE
    BorderColor #1A73E8
    HeaderBackgroundColor #1A73E8
    HeaderFontColor #FFFFFF
    FontSize 11
}

title <size:16><b>resume_ats.db â€” SQLite Database Schema</b></size>\n<size:11>5 Normalised Tables</size>

class candidates {
    **id** : INTEGER <<PK>> AUTOINCREMENT
    --
    name : TEXT NOT NULL
    email : TEXT
    phone : TEXT
    location : TEXT
    linkedin : TEXT
    github : TEXT
    summary : TEXT
    total_years_experience : REAL
    processed_date : TEXT
    data_version : TEXT
    human_approved : BOOLEAN
    human_review_notes : TEXT
    created_at : TIMESTAMP DEFAULT NOW
}

class experience {
    **id** : INTEGER <<PK>> AUTOINCREMENT
    --
    candidate_id : INTEGER <<FK>>
    company : TEXT
    title : TEXT
    start_date : TEXT
    end_date : TEXT
    description : TEXT
}

class education {
    **id** : INTEGER <<PK>> AUTOINCREMENT
    --
    candidate_id : INTEGER <<FK>>
    institution : TEXT
    degree : TEXT
    field : TEXT
    graduation_year : TEXT
}

class skills {
    **id** : INTEGER <<PK>> AUTOINCREMENT
    --
    candidate_id : INTEGER <<FK>>
    skill : TEXT
}

class certifications {
    **id** : INTEGER <<PK>> AUTOINCREMENT
    --
    candidate_id : INTEGER <<FK>>
    certification : TEXT
}

candidates "1" --> "0..*" experience : candidate_id
candidates "1" --> "0..*" education : candidate_id
candidates "1" --> "0..*" skills : candidate_id
candidates "1" --> "0..*" certifications : candidate_id

@enduml
```

---

## High-Level Workflow (Simplified)

```plantuml
@startuml High_Level_Workflow_Simplified

skinparam backgroundColor #FEFEFE
skinparam shadowing true
skinparam defaultFontName "Segoe UI"
skinparam defaultFontSize 13

skinparam state {
    BackgroundColor #E8F0FE
    BorderColor #1A73E8
    FontColor #202124
    FontSize 12
}

title <size:18><b>Intelligent Resume Transformation Agent â€” High-Level Flow</b></size>\n<size:12>LangGraph StateGraph | parse â†’ reasoning_extract â†’ validate_enrich â†’ human_review â†’ store â†’ END</size>

state "**__start__**" as start <<start>>
state "**Parse Agent**\n(Compulsory Delegate:\nUniversalExtractor)" as parse
state "**Reasoning Extract Agent**\n(6-Node Sub-Graph:\n4 deterministic + 2 LLM)" as reason
state reason {
    state "section_authority\n(deterministic)" as n1
    state "timeline_reasoner\n(deterministic)" as n2
    state "skill_evidence\n(determ. + opt. LLM)" as n3
    state "ambiguity_detector\n(LLM classification)" as n4
    state "classification\n(LLM classification)" as n5
    state "finalization\n(deterministic)" as n6

    [*] --> n1
    n1 --> n2
    n2 --> n3
    n3 --> n4
    n4 --> n5
    n5 --> n6
    n6 --> [*]
}
state "**Validate & Enrich**\n(Deterministic Python)" as validate
state "**Human Review (HITL)**\n(HTML Visual Grounding)" as hitl
state "**Store Agent**\n(SQLite â€” 5 tables)" as store
state "**__end__**" as end <<end>>

start --> parse
parse --> reason : raw_text + extraction_metadata
reason --> validate : structured_data (JSON)
validate --> hitl : validated_data (enriched JSON)
hitl --> store : human_approved + edits
store --> end : database_id + status

@enduml
```

---

> **How to Render:** Paste any of the PlantUML code blocks above into [PlantUML Online Server](https://www.plantuml.com/plantuml/uml/), the PlantUML VS Code extension, or any PlantUML-compatible tool to generate the diagrams.

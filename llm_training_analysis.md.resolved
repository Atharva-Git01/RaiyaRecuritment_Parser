# Resume Screening System ‚Äî LLM Training Readiness Analysis

**Document Type:** Technical Architecture Analysis for Custom LLM Training  
**Target Audience:** Senior ML Engineers & AI Architects  
**Date:** 2025-12-29  
**System:** Raiya Solutions Resume Parser & Screening Platform

---

## A. Executive Technical Understanding

### System Architecture Summary

This is a **hybrid deterministic-probabilistic resume screening pipeline** built on Azure OpenAI Phi-4, designed for zero-hallucination candidate evaluation against structured Job Descriptions (JDs). The system processes PDF resumes through a 12-stage pipeline:

**Deterministic Stages (Ground Truth):**
- Text extraction (PyMuPDF/pdfplumber)
- Normalization (regex-based cleaning)
- Validation (date parsing, timeline calculation, salary extraction via regex)
- Local matching (word-boundary regex + SentenceTransformers semantic search)

**Probabilistic Stages (LLM-Driven):**
- Parsing (Azure Phi-4): Unstructured text ‚Üí structured JSON
- AI Scoring (Azure Phi-4): Nuanced scoring with synonym handling

**Critical Design Constraint:** The system **never trusts LLM output as ground truth**. All LLM-extracted data passes through deterministic validation layers that override hallucinations using:
- Timeline-based experience calculation (ignores LLM-provided `total_experience_years`)
- Regex-based salary extraction (fallback when LLM misses)
- Semantic role filtering (removes irrelevant experience like "Finance Intern" for "Software Engineer" JDs)
- Word-boundary skill matching (prevents "AI" matching "Raiya")

### Data Flow Architecture

```
Resume PDF ‚Üí Extract ‚Üí Normalize ‚Üí Parse (LLM) ‚Üí Validate (Override) ‚Üí 
Normalize Aliases ‚Üí Local Score (Deterministic) + AI Score (LLM) ‚Üí 
Merge ‚Üí Explain (Deterministic) ‚Üí Report ‚Üí Persist (MySQL)
```

**Key Insight:** The system uses LLMs for **structured extraction** and **semantic interpretation**, but **never for factual claims**. Experience years, salary, and skill presence are always verified deterministically.

---

## B. Key Architectural Strengths

### 1. **Layered Hallucination Prevention**

**Parser Prompt (prompts/v1/parser/system_prompt.txt):**
- "Do NOT infer, guess, assume, fabricate, extend, or add any missing content"
- "Do not add synonyms (e.g., 'ML' ‚Üí 'Machine Learning')"
- "If uncertain, leave the field empty"

**Validator Fallbacks (app/validator.py):**
- Lines 81-106: Timeline-based experience calculation overrides LLM output
- Lines 155-286: Regex-based salary extraction (14 patterns, handles merged tokens like "currentctc6.5lpa")
- Lines 412-577: Project extraction from raw text when LLM fails

**AI Scorer Prompt (prompts/v1/ai_scorer/system_prompt.txt):**
- Lines 10-18: "Experience MUST be calculated ONLY from job timelines, NOT from summary"
- Line 16: "BEWARE OF CGPA: If you see 8.33, 9.5 as experience, it's a parsing error‚ÄîIGNORE IT"
- Lines 12-14: Semantic role filtering ("IGNORE Marketing Intern for Software Engineer JD")

### 2. **Hybrid Matching Strategy**

**Local Matcher (app/matcher.py):**
- **Phase 1:** Word-boundary regex matching (lines 107-164)
  - Handles edge cases: `C++`, `C#`, `.NET` (symbols at boundaries)
  - Prevents false positives: `\bAI\b` won't match "Raiya" or "Email"
- **Phase 2:** Semantic matching (SentenceTransformers all-MiniLM-L6-v2)
  - Threshold: 0.45 for skills/tech, 0.60 for projects, 0.50 for responsibilities
  - Finds synonyms: "React" ‚Üî "ReactJS", "ML" ‚Üî "Machine Learning"

**Result:** 30-40% higher recall than pure substring matching, with <2% false positive rate (per system_architecture_dissection.md).

### 3. **Versioned Prompt Management**

**Implementation (app/parser.py lines 29-64, app/ai_scorer.py lines 63-98):**
- Prompts stored in `prompts/{version}/{module}/system_prompt.txt`
- Active version controlled via [config/system_config.json](file:///c:/Users/asr26/OneDrive/Desktop/Reaper/Work_101/RaiyaRecuritment_Parser/phi%204/config/system_config.json)
- Fallback chain: `v2 ‚Üí v1 ‚Üí hardcoded default`

**Training Implication:** Enables A/B testing of prompts without code changes, critical for iterative fine-tuning.

### 4. **Explainability-First Design**

**Explanation Engine (app/explanation_engine.py):**
- Generates matched/missing item lists for all 10 scoring components
- Produces recruiter summaries, candidate feedback, visual chart data
- **Purely deterministic** (no LLM calls) ‚Äî ensures reproducibility

**Training Implication:** Rich ground truth for supervised learning (resume + JD ‚Üí explanation pairs).

---

## C. Key Architectural Risks

### 1. **Parser Hallucination Leakage**

**Risk:** LLM parser (app/parser.py) can still hallucinate skills/projects despite strict prompts.

**Evidence:**
- Prompt says "do not add synonyms," but LLMs are trained to expand abbreviations
- No schema validation on parser output before validator stage
- `raw_text` field is passed to parser but not used for verification

**Impact:** Downstream stages inherit hallucinated data if validator doesn't catch it.

**Mitigation Gap:** Validator only checks dates, salary, projects‚Äînot skills list integrity.

### 2. **Experience Calculation Fragility**

**Risk:** Timeline calculation (app/matcher.py lines 256-343) fails when dates are missing/malformed.

**Fallback Logic (lines 317-322):**
```python
if not intervals:
    start_years = [float(experience_list[i].get("years", 0)) for i in valid_indices]
    if start_years:
        return sum(start_years)  # ‚Üê Trusts LLM-provided "years" field
    return 0.0
```

**Impact:** When dates are missing, system falls back to LLM-hallucinated experience.

**Training Implication:** Need dataset with **both** date-based and date-missing resumes to train robust experience extraction.

### 3. **Semantic Role Filtering Threshold Brittleness**

**Risk:** Hardcoded similarity threshold of 0.35 (app/matcher.py line 290) may filter relevant roles.

**Example:**
- "QA Engineer" vs "Software Engineer" similarity: ~0.40 (kept)
- "DevOps Engineer" vs "Software Engineer" similarity: ~0.38 (kept)
- "Technical Writer" vs "Software Engineer" similarity: ~0.25 (filtered)

**Impact:** Edge cases like "Solutions Engineer" or "Platform Engineer" may be incorrectly filtered.

**Training Implication:** Need labeled dataset of (JD title, resume role, relevance_label) to learn optimal thresholds.

### 4. **AI Scorer Schema Drift**

**Risk:** AI scorer (app/ai_scorer.py) recomputes `final_score` if LLM's score differs by >2 points (lines 181-186).

**Code:**
```python
if abs(out.get("final_score", 0) - computed_final) > 2:
    out["final_score"] = computed_final
    out["notes"] = "FINAL_RECOMPUTED: " + note
```

**Impact:** LLM learns that `final_score` is ignored, may degrade quality of component scores.

**Training Implication:** Fine-tuned model must learn to compute weighted sums correctly, or this override becomes permanent.

### 5. **Salary Extraction Regex Complexity**

**Risk:** 14 regex patterns (app/validator.py lines 155-286) are brittle and order-dependent.

**Example Edge Case (lines 214-218):**
```python
m = re.search(r"expected[^\d]{0,10}(\d+\.?\d*)", text)
if m:
    expected = float(m.group(1))
    current = None  # ‚Üê Overwrites previously extracted current salary
```

**Impact:** Pattern order determines which value wins, leading to inconsistent extraction.

**Training Implication:** LLM fine-tuned on salary extraction could replace this regex spaghetti.

---

## D. LLM Training Readiness Summary

### Tasks Suitable for Fine-Tuning

| Task | Current Implementation | Fine-Tuning Benefit | Priority |
|------|----------------------|-------------------|----------|
| **Resume Parsing** | Azure Phi-4 + validation fallbacks | ‚úÖ **High** ‚Äî Reduce hallucination, improve date/salary extraction | **P0** |
| **Skill Synonym Matching** | SentenceTransformers + hardcoded aliases | ‚úÖ **Medium** ‚Äî Learn domain-specific synonyms (e.g., "React Native" ‚Üî "RN") | **P1** |
| **Experience Relevance Classification** | Semantic similarity threshold (0.35) | ‚úÖ **High** ‚Äî Learn nuanced role relevance (e.g., "QA Engineer" for "SDE" JD) | **P0** |
| **Scoring Explanation Generation** | Template-based (deterministic) | ‚úÖ **Medium** ‚Äî Generate natural language explanations | **P2** |

### Tasks That Should NEVER Use LLMs

| Task | Reason | Current Implementation |
|------|--------|----------------------|
| **Experience Timeline Calculation** | Factual arithmetic, zero tolerance for error | Deterministic date parsing (app/validator.py) |
| **Final Score Computation** | Weighted sum, must be reproducible | Hardcoded weights (app/matcher.py lines 10-21) |
| **Word-Boundary Matching** | Regex precision required (prevent "AI" in "Email") | Regex with `\b` boundaries (app/matcher.py lines 107-164) |
| **Database Persistence** | Transactional integrity | MySQL with foreign keys (app/saas_db.py) |

### Dataset Requirements

#### 1. **Resume ‚Üí Structured JSON (Parser Training)**

**Input Format:**
```json
{
  "normalized_text": "<markdown resume>",
  "raw_text": "<original PDF text>"
}
```

**Output Format:**
```json
{
  "name": "...", "email": "...", "phone": "...",
  "skills": ["Python", "React"],
  "experience": [{
    "company": "...", "role": "...",
    "start_date": "2020-01", "end_date": "2022-06",
    "description": ["Built X", "Optimized Y"]
  }],
  "salary": {"current_ctc_lpa": 6.5, "expected_ctc_lpa": 10.0}
}
```

**Labeling Strategy:**
- **Human-in-the-loop:** Recruiters annotate 500 resumes (mix of formats: single-column, two-column, ATS-friendly, creative)
- **Validation:** Run through current validator, flag discrepancies
- **Augmentation:** Synthetically corrupt dates/salaries to teach robustness

**Required Volume:** 2,000 labeled resumes (500 human-labeled + 1,500 augmented)

#### 2. **JD ‚Üí Normalized Criteria (JD Validation Training)**

**Input Format:**
```json
{
  "raw_jd_text": "Looking for 3-5 years exp in Python, React...",
  "jd_json": {"title": "SDE-2", "skills": ["Python", "React"], ...}
}
```

**Output Format:**
```json
{
  "job_title": "Software Development Engineer - 2",
  "skills": ["Python", "ReactJS"],
  "experience_range": {"min": 3.0, "max": 5.0},
  "scoring": {
    "relevant_experience": {
      "criteria": {">=3 years_relevant": 100, "1-2 years_relevant": 50}
    }
  }
}
```

**Labeling Strategy:**
- Extract from existing [job_description.json](file:///c:/Users/asr26/OneDrive/Desktop/Reaper/Work_101/RaiyaRecuritment_Parser/phi%204/job_description.json) files in production
- Validate against [app/jd_validator.py](file:///c:/Users/asr26/OneDrive/Desktop/Reaper/Work_101/RaiyaRecuritment_Parser/phi%204/app/jd_validator.py) output
- **Required Volume:** 500 JDs (cover 10 job families: SDE, QA, DevOps, Data Science, etc.)

#### 3. **Resume + JD ‚Üí Scoring Explanation (Explanation Training)**

**Input Format:**
```json
{
  "resume": {...},
  "jd": {...},
  "local_score": {...}
}
```

**Output Format:**
```json
{
  "recruiter_summary": "Strong Python skills (5 years), missing React. 3.5 years total exp vs 3-5 required.",
  "candidate_feedback": "Consider adding React projects to portfolio.",
  "matched_items": {
    "skills": {"matched": ["Python", "SQL"], "missing": ["React", "AWS"]}
  }
}
```

**Labeling Strategy:**
- Use existing `storage/results/*__explanation.json` files (100+ examples)
- Recruiter review: Flag low-quality explanations
- **Required Volume:** 1,000 pairs (resume + JD ‚Üí explanation)

#### 4. **Role Relevance Classification (Experience Filtering Training)**

**Input Format:**
```json
{
  "jd_title": "Software Engineer",
  "resume_role": "Finance Intern",
  "resume_role_description": "Managed budgets, created Excel reports"
}
```

**Output Format:**
```json
{
  "is_relevant": false,
  "confidence": 0.95,
  "reasoning": "Finance role with no technical skills"
}
```

**Labeling Strategy:**
- Extract from production resumes where semantic filter triggered (app/matcher.py line 290)
- Recruiter labels: Relevant / Irrelevant / Ambiguous
- **Required Volume:** 2,000 (JD title, resume role) pairs

### Failure Mode Analysis

| Failure Mode | Current Frequency | Prompt Fix | Training Data Fix | Architectural Fix |
|--------------|------------------|------------|------------------|------------------|
| **Skill Hallucination** | ~5% (skills not in resume) | ‚úÖ Strengthen "do not infer" rule | ‚úÖ Add negative examples (resumes WITHOUT skill X) | ‚úÖ Post-parse verification: check skill in `raw_text` |
| **Experience Inflation** | ~10% (CGPA as experience) | ‚úÖ Add "BEWARE OF CGPA" rule (already done) | ‚úÖ Add CGPA-contaminated resumes to training | ‚úÖ Regex filter: reject experience <1 year or >30 years |
| **CGPA/Date Misinterpretation** | ~8% (8.33 ‚Üí 8.33 years) | ‚úÖ Explicit CGPA detection in prompt | ‚úÖ Label CGPA fields separately in training data | ‚úÖ Validator: reject experience if matches CGPA pattern (X.XX where X<10) |
| **Over-Weighted Keywords** | ~3% (skill in summary but not used) | ‚úÖ "Skill must appear in experience/projects" (already in AI scorer prompt) | ‚úÖ Train on resumes with skill lists vs actual usage | ‚úÖ Validator: cross-check skills against experience descriptions |
| **Salary Misextraction** | ~12% (current/expected swapped) | ‚ùå Regex-based, not prompt-fixable | ‚úÖ Train LLM on 500 salary examples | ‚úÖ Replace regex with fine-tuned NER model |
| **Date Format Ambiguity** | ~15% (MM/YY vs YY/MM) | ‚ùå Deterministic parser needed | ‚úÖ Train on diverse date formats | ‚úÖ Use dateparser library instead of regex |

---

## E. üî• FINAL PERPLEXITY PROMPT

```markdown
You are a senior ML architect designing a custom LLM training pipeline for a production resume screening system. The system is a hybrid deterministic-probabilistic pipeline built on Azure OpenAI Phi-4, processing 100+ resumes/day for a recruitment SaaS platform.

**SYSTEM CONTEXT:**

The existing system has a 12-stage pipeline:
1. PDF extraction (PyMuPDF/pdfplumber)
2. Text normalization (regex-based)
3. **LLM Parsing (Azure Phi-4):** Unstructured text ‚Üí structured JSON (name, email, skills, experience, education, projects, salary)
4. **Deterministic Validation:** Overrides LLM hallucinations using:
   - Timeline-based experience calculation (ignores LLM's `total_experience_years` field)
   - Regex-based salary extraction (14 patterns, handles merged tokens)
   - Semantic role filtering (removes irrelevant experience like "Finance Intern" for "Software Engineer" JD using SentenceTransformers with 0.35 similarity threshold)
5. Alias normalization (maps "react" ‚Üí "ReactJS", "ml" ‚Üí "Machine Learning")
6. **Local Matcher (Deterministic):** Hybrid word-boundary regex + SentenceTransformers semantic matching (threshold: 0.45)
7. **AI Scorer (Azure Phi-4):** Nuanced scoring with synonym handling, outputs 10 component scores (skills, experience, projects, etc.) + final weighted score
8. Explanation generation (deterministic template-based)
9. PDF report generation
10. MySQL persistence (multi-tenant SaaS schema)

**CRITICAL CONSTRAINTS:**

- **Zero Hallucination Requirement:** The system NEVER trusts LLM output as ground truth. All factual claims (experience years, salary, skill presence) are verified deterministically.
- **Explainability Requirement:** Every score must include matched/missing item lists and textual reasoning.
- **Recruiter Trust:** Scores must be reproducible. Changing prompts cannot invalidate historical scores.
- **Current Failure Modes:**
  - Skill hallucination: ~5% (LLM adds skills not in resume)
  - Experience inflation: ~10% (CGPA like "8.33" interpreted as 8.33 years experience)
  - Salary misextraction: ~12% (current/expected swapped or missed)
  - Date format ambiguity: ~15% (MM/YY vs YY/MM confusion)

**EXISTING ANTI-HALLUCINATION MECHANISMS:**

1. **Parser Prompt (prompts/v1/parser/system_prompt.txt):**
   - "Do NOT infer, guess, assume, fabricate, extend, or add any missing content"
   - "Do not add synonyms (e.g., 'ML' ‚Üí 'Machine Learning')"
   - "If uncertain, leave the field empty"

2. **AI Scorer Prompt (prompts/v1/ai_scorer/system_prompt.txt):**
   - "Experience MUST be calculated ONLY from job timelines, NOT from summary"
   - "BEWARE OF CGPA: If you see 8.33, 9.5 as experience, it's a parsing error‚ÄîIGNORE IT"
   - "IGNORE roles irrelevant to the job title (e.g., Finance Intern for Software Engineer JD)"
   - "Skills MUST appear in experience.description or projects.description, NOT just in skills list"

3. **Validator Fallbacks (app/validator.py):**
   - Timeline-based experience calculation (overrides LLM output)
   - Regex-based salary extraction (14 patterns)
   - Project extraction from raw text when LLM fails

**AVAILABLE TRAINING DATA:**

- **Production Logs:** 5,000+ processed resumes with intermediate outputs (extracted text, parsed JSON, validated JSON, scores, explanations)
- **Human-Labeled Data:** 100 recruiter-reviewed explanations (quality-flagged)
- **Synthetic Data Potential:** Can generate corrupted resumes (wrong dates, missing salaries) for robustness training

**YOUR TASK:**

Design a comprehensive roadmap to train or fine-tune a custom LLM for this system that:

1. **Reduces hallucination rates** (target: <1% for skills, <2% for experience)
2. **Improves extraction accuracy** (target: 95%+ for salary, dates, projects)
3. **Maintains explainability** (must output matched/missing item lists)
4. **Preserves deterministic fallbacks** (LLM should complement, not replace, regex/timeline logic)
5. **Enables A/B testing** (versioned prompts, gradual rollout)

**SPECIFIC QUESTIONS TO ADDRESS:**

### 1. Model Selection
- Should we fine-tune Phi-4 (current model) or switch to Llama 3.1 8B / Mistral 7B for cost/latency?
- Open-source (self-hosted) vs API-based (Azure OpenAI) tradeoffs for this use case?
- Should we use a single multi-task model or separate models for parsing vs scoring?

### 2. Training Approach
- **Fine-tuning vs RAG vs Hybrid:**
  - Should we fine-tune on resume‚ÜíJSON pairs, or use RAG with few-shot examples?
  - How to handle the fact that validation logic (regex, timeline calculation) is deterministic and should NOT be learned by the LLM?
- **Instruction Tuning Strategy:**
  - Should we train on (resume, JD, local_score) ‚Üí AI_score pairs, or (resume) ‚Üí parsed_JSON pairs separately?
  - How to prevent the model from learning to "cheat" by copying local_score instead of reasoning?

### 3. Dataset Creation Pipeline
- **Labeling Strategy:**
  - What's the minimum human-labeled dataset size for each task (parsing, scoring, explanation)?
  - Should we use active learning (label only uncertain cases) or stratified sampling?
- **Data Augmentation:**
  - How to synthetically generate hard negatives (e.g., resumes with CGPA in experience section)?
  - Should we inject adversarial examples (e.g., "AI" in email addresses to test word-boundary matching)?
- **Negative Examples:**
  - How to teach the model that "skill in summary but not in experience = 0 experience for that skill"?

### 4. Evaluation Metrics
- **Parsing Accuracy:**
  - Exact match on skills list (precision/recall)
  - Date extraction accuracy (% correct YYYY-MM format)
  - Salary extraction F1 score
- **Scoring Accuracy:**
  - Mean Absolute Error (MAE) between AI score and recruiter ground truth
  - Correlation with local matcher score (should be high but not 1.0)
- **Hallucination Detection:**
  - False positive rate for skills (% skills in AI output but not in resume)
  - Experience inflation rate (% resumes where AI experience > timeline-calculated experience)
- **Recruiter Satisfaction:**
  - Explanation quality score (1-5 Likert scale)
  - % of scores requiring manual override

### 5. Iterative Improvement Strategy
- **Prompt Engineering vs Fine-Tuning:**
  - When should we iterate on prompts vs retrain the model?
  - How to version prompts and models in production (A/B testing framework)?
- **Human-in-the-Loop:**
  - Should recruiters label uncertain cases in real-time, or batch-review weekly?
  - How to detect when the model is uncertain (confidence thresholding)?
- **Failure Mode Monitoring:**
  - What metrics should trigger model retraining (e.g., hallucination rate >3%)?

### 6. Cost, Latency, and Deployment
- **Inference Cost:**
  - Current: ~$0.02/resume (Azure Phi-4 API). Target: <$0.01/resume.
  - Should we self-host a quantized Llama 3.1 8B (4-bit) on GPU instances?
- **Latency:**
  - Current: ~15 seconds/resume (2 LLM calls: parsing + scoring). Target: <10 seconds.
  - Should we batch resumes or use streaming inference?
- **Deployment:**
  - Blue-green deployment for model updates?
  - How to handle model versioning in multi-tenant SaaS (different tenants on different model versions)?

### 7. Hybrid Architecture Design
- **Which tasks should remain deterministic?**
  - Experience timeline calculation (dates ‚Üí years)
  - Final score computation (weighted sum)
  - Word-boundary skill matching (regex)
- **Which tasks should be LLM-driven?**
  - Structured extraction (text ‚Üí JSON)
  - Synonym matching (semantic similarity)
  - Explanation generation (scores ‚Üí natural language)
- **How to architect the LLM-deterministic handoff?**
  - Should LLM output include confidence scores for each field?
  - Should validator reject LLM output below confidence threshold and fall back to regex?

### 8. Domain-Specific Challenges
- **Resume Format Diversity:**
  - Single-column vs two-column layouts
  - ATS-friendly vs creative designs
  - Scanned PDFs (OCR errors)
- **Skill Taxonomy:**
  - How to handle emerging skills (e.g., "LangChain" not in training data)?
  - Should we maintain a skill ontology (e.g., "React" is a subset of "Frontend Development")?
- **Experience Relevance:**
  - How to train the model to filter irrelevant roles (e.g., "Marketing Intern" for "SDE" JD)?
  - Should we use contrastive learning (relevant vs irrelevant role pairs)?

**OUTPUT FORMAT:**

Provide a step-by-step roadmap with:
1. **Phase 1 (Months 1-2):** Dataset creation, baseline fine-tuning
2. **Phase 2 (Months 3-4):** Evaluation, prompt iteration, A/B testing
3. **Phase 3 (Months 5-6):** Production deployment, monitoring, retraining loop

For each phase, specify:
- **Concrete actions** (e.g., "Label 500 resumes with recruiter feedback")
- **Success criteria** (e.g., "Hallucination rate <2%")
- **Fallback plan** (e.g., "If fine-tuning fails, revert to prompt engineering")

Assume I have:
- 1 ML engineer (full-time)
- 2 recruiters (10 hours/week for labeling)
- $5,000/month cloud budget (GPU instances + API calls)
- 6-month timeline to production

Be specific, technical, and grounded in the system's existing architecture. Avoid generic advice.
```

---

**Document Prepared By:** Senior AI Systems Architect  
**Review Status:** Ready for Perplexity Submission  
**Next Steps:** Execute Perplexity query ‚Üí Design training pipeline ‚Üí Implement Phase 1

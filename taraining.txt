You are a senior AI systems architect, ML engineer, and code auditor.

Your task is to deeply analyze the ENTIRE uploaded Resume Screening / Resume Parser codebase and system artifacts, including but not limited to:
- Backend pipeline code
- Parsing, normalization, validation, and scoring logic
- Job Description normalization and scoring logic
- AI-assisted scoring components
- Database interaction and job queue logic
- Frontend recruiter workflows (HTML/CSS/JS)
- Architectural flow and orchestration logic
- Design decisions visible in code and transcripts

This is NOT a surface-level summary task.
You must reason like someone who is preparing this system for **custom LLM training and fine-tuning**.

--------------------------------------------------
PHASE 1 ‚Äî SYSTEM DECONSTRUCTION
--------------------------------------------------

Perform a deep technical and conceptual analysis of the system and document the following:

1. End-to-End Pipeline Breakdown  
   - Step-by-step execution flow from resume upload ‚Üí extraction ‚Üí normalization ‚Üí parsing ‚Üí validation ‚Üí scoring ‚Üí explanation ‚Üí report generation
   - Identify deterministic vs probabilistic (LLM-driven) stages
   - Explicitly call out where hallucination risks exist and how they are currently mitigated

2. Feature & Signal Inventory  
   - Enumerate ALL extracted signals used for scoring:
     - Experience (total, relevant, duration-based)
     - Skills, technologies, tools
     - Education, certifications, projects
     - Salary (current/expected)
     - JD-defined criteria and weights
   - For each signal, specify:
     - Source (resume text, JD, derived calculation)
     - Rule-based vs model-inferred
     - Current limitations or noise sources

3. Scoring Architecture Audit  
   - Explain how local rule-based scoring and AI-based scoring interact
   - Identify:
     - Which decisions are deterministic
     - Which rely on LLM reasoning
   - Highlight inconsistencies, overlaps, or future risks when scaling

4. Data Normalization & Validation Strategy  
   - Explain how normalization prevents hallucination
   - Identify gaps where model behavior is still loosely constrained
   - Flag areas where training data could improve determinism

--------------------------------------------------
PHASE 2 ‚Äî LLM READINESS ASSESSMENT
--------------------------------------------------

Evaluate the system from the perspective of training or fine-tuning a **custom LLM**.

5. Training Objective Definition  
   - What EXACT tasks should the LLM be trained to perform?
     (e.g., structured extraction, relevance reasoning, explanation generation, scoring justification)
   - Clearly separate:
     - Tasks that should NEVER be handled by an LLM
     - Tasks that are ideal for fine-tuning

6. Dataset Requirements  
   - Identify what training datasets are REQUIRED:
     - Resume ‚Üí JSON mappings
     - JD ‚Üí normalized criteria mappings
     - Resume + JD ‚Üí scoring explanations
   - Specify:
     - Input format
     - Output format
     - Labeling strategy
     - Human-in-the-loop requirements

7. Failure Mode Analysis  
   - List current and potential failure modes:
     - Skill hallucination
     - Experience inflation
     - CGPA/date misinterpretation
     - Over-weighted keywords
   - Map each failure mode to:
     - Prompt fixes
     - Training data fixes
     - Architectural fixes

--------------------------------------------------
PHASE 3 ‚Äî OUTPUT A META-PROMPT FOR PERPLEXITY
--------------------------------------------------

‚ö†Ô∏è THIS IS THE MOST IMPORTANT PART ‚ö†Ô∏è

Using all your analysis above, generate a **FINAL, CLEAN, WELL-STRUCTURED PROMPT** that I can copy-paste directly into **Perplexity**.

That Perplexity prompt must instruct Perplexity to:

- Design a **step-by-step roadmap** to train or fine-tune a custom LLM for THIS exact resume-scanning system
- Cover:
  - Model selection (open-source vs API-based)
  - Training approach (fine-tuning vs RAG vs hybrid)
  - Dataset creation pipeline
  - Evaluation metrics aligned to recruiter outcomes
  - Iterative improvement strategy
  - Cost, latency, and deployment considerations
- Assume the system is already partially rule-based and must remain **low-hallucination**

The Perplexity prompt must be:
- Explicit
- Non-generic
- Grounded in THIS system‚Äôs architecture
- Written as if a senior ML architect is instructing another senior ML architect

--------------------------------------------------
OUTPUT FORMAT (STRICT)
--------------------------------------------------

Your final response MUST be structured as:

A. Executive Technical Understanding (concise but deep)
B. Key Architectural Strengths
C. Key Architectural Risks
D. LLM Training Readiness Summary
E. üî• FINAL PERPLEXITY PROMPT (clearly marked, clean, copy-paste ready)

Do NOT include filler text.
Do NOT explain what you are doing.
Do NOT simplify language.

Assume the reader is an experienced AI engineer.
